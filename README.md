<div align="center">

<br/>

```
â–ˆâ–ˆâ•—     â–ˆâ–ˆâ•—     â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—     â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘    â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•
â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘    â–ˆâ–ˆâ•”â•â•â•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â• â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘    â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•     â•šâ•â•    â•šâ•â•     â•šâ•â•â•šâ•â•     â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•â•â•šâ•â•â•â•â•â•â•
```

# LLM Data Preprocessing, Embedding & Architecture

**A comprehensive hands-on Jupyter notebook exploring the foundational internals of Large Language Models â€” from raw text to token embeddings to Transformer architecture.**

<br/>

[![Python](https://img.shields.io/badge/Python-3.9%2B-3776AB?style=flat-square&logo=python&logoColor=white)](https://www.python.org/)
[![Jupyter](https://img.shields.io/badge/Jupyter-Notebook-F37626?style=flat-square&logo=jupyter&logoColor=white)](https://jupyter.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C?style=flat-square&logo=pytorch&logoColor=white)](https://pytorch.org/)
[![Transformers](https://img.shields.io/badge/HuggingFace-Transformers-FFD21E?style=flat-square)](https://huggingface.co/transformers/)
[![tiktoken](https://img.shields.io/badge/tiktoken-OpenAI-412991?style=flat-square)](https://github.com/openai/tiktoken)
[![License: MIT](https://img.shields.io/badge/License-MIT-22c55e?style=flat-square)](LICENSE)
[![Notebook](https://img.shields.io/badge/Open-Notebook-orange?style=flat-square&logo=jupyter)](./LLM_Data_Preprocessing.ipynb)

<br/>

> *"To understand how large language models think, you must first understand how they see."*

<br/>

</div>

---

## ğŸ“– Overview

This repository contains a structured, educational deep-dive into the **data pipeline and internal mechanics that power modern Large Language Models (LLMs)** such as GPT, LLaMA, and BERT.

Rather than treating LLMs as black boxes, this notebook tears them open â€” walking through every stage from raw corpus to the final embedding space. It is built as both a **learning resource** and a **reference implementation** for anyone who wants to understand what truly happens before a model ever sees a forward pass.

The notebook is organized around three core pillars:

| Pillar | What You'll Build | Concepts Covered |
|--------|-------------------|------------------|
| ğŸ§¹ **Data Preprocessing** | A clean, structured NLP pipeline | Tokenization, normalization, BPE, sliding windows |
| ğŸ”¢ **Embeddings** | Token + positional embedding layers | Word2Vec, token IDs, positional encodings |
| ğŸ—ï¸ **Architecture** | GPT-style Transformer components | Self-attention, multi-head attention, feed-forward layers |

---

## ğŸ¯ What This Notebook Covers

### Stage 1 â€” Raw Text Preprocessing

The journey begins with unstructured text and ends with a clean, model-ready corpus.

```
Raw Text  â†’  Normalization  â†’  Tokenization  â†’  Token IDs  â†’  Sliding Window Batches
```

- **Text normalization** â€” lowercasing, punctuation handling, Unicode cleanup
- **Tokenization strategies** â€” character-level, word-level, and subword comparison
- **Byte Pair Encoding (BPE)** â€” the algorithm behind GPT's `tiktoken` tokenizer
- **Vocabulary construction** â€” building and mapping a `{token: id}` lookup table
- **Sliding window dataset** â€” generating `(input, target)` sequence pairs with configurable `context_length` and `stride`

---

### Stage 2 â€” Embedding Layer

Converts discrete token IDs into continuous, trainable vector representations.

```
Token IDs  â†’  Token Embedding Matrix  â†’  + Positional Encoding  â†’  Embedded Sequence
```

- **Token embeddings** â€” a learnable `nn.Embedding(vocab_size, d_model)` layer mapping each token to a dense vector
- **Positional encodings** â€” both learned (GPT-style) and sinusoidal approaches, injecting sequence-order information
- **Embedding visualization** â€” how semantically similar tokens cluster in the embedding space
- **Combined input representation** â€” the final `token_embedding + position_embedding` tensor fed into the Transformer

---

### Stage 3 â€” Transformer Architecture

Implements the core building blocks of a decoder-only (GPT-style) Transformer from scratch in PyTorch.

```
Embedded Input  â†’  Multi-Head Self-Attention  â†’  FFN  â†’  Layer Norm  â†’  Residual  â†’  Output Logits
```

- **Scaled dot-product attention** â€” `softmax(QKáµ€ / âˆšd_k) Â· V` implemented step-by-step
- **Causal masking** â€” ensuring each token only attends to previous positions during text generation
- **Multi-head attention** â€” splitting queries, keys, and values across `n_head` independent attention heads
- **Feed-forward sublayer** â€” the expand-then-compress MLP with GELU activation
- **Layer normalization & residual connections** â€” stabilizing gradients in deep networks
- **Stacked Transformer blocks** â€” assembling the full model from individual composable components

---

## ğŸ—‚ï¸ Repository Structure

```
LLM-Data-Preprocessing-Embedding-Architecture-/
â”‚
â”œâ”€â”€ ğŸ““ LLM_Data_Preprocessing.ipynb   â† Main notebook (all 3 stages)
â”œâ”€â”€ ğŸ“„ README.md                       â† This file
â”œâ”€â”€ ğŸš« .gitignore
â””â”€â”€ âš™ï¸  .gitattributes
```

The entire implementation lives in a single well-structured notebook with clearly separated sections, markdown explanations for every concept, and inline code comments throughout.

---

## ğŸš€ Quick Start

### Prerequisites

Make sure you have **Python 3.9+** and **pip** installed.

### 1 â€” Clone the repository

```bash
git clone https://github.com/Minhaj078/LLM-Data-Preprocessing-Embedding-Architecture-.git
cd LLM-Data-Preprocessing-Embedding-Architecture-
```

### 2 â€” Install dependencies

```bash
# Core ML stack
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# NLP + notebook tools
pip install transformers tiktoken numpy matplotlib scikit-learn tqdm jupyterlab
```

> **CPU-only machines:** Replace the PyTorch install URL with `https://download.pytorch.org/whl/cpu`

### 3 â€” Launch the notebook

```bash
jupyter lab LLM_Data_Preprocessing.ipynb
```

Or with classic Jupyter:

```bash
jupyter notebook LLM_Data_Preprocessing.ipynb
```

### 4 â€” Run all cells

Use `Kernel â†’ Restart & Run All` to execute the full pipeline end-to-end.

---

## ğŸ§  Key Concepts Explained

### Byte Pair Encoding (BPE)

BPE is the subword tokenization algorithm used by GPT-2, GPT-3, GPT-4, and most modern LLMs. It starts with character-level tokens and iteratively merges the most frequent adjacent pairs until a target vocabulary size is reached.

```
"tokenization" â†’ ["token", "ization"]    # BPE splits at learned boundaries
"untokenized"  â†’ ["un", "token", "ized"] # Handles unseen words gracefully
```

This approach allows the model to handle rare words, out-of-vocabulary terms, and new languages without collapsing to generic `[UNK]` tokens.

---

### Why Positional Encodings?

The self-attention mechanism is inherently **position-agnostic** â€” it sees a set of tokens, not an ordered sequence. Positional encodings inject order information directly into each token's embedding vector.

```python
# Sinusoidal (fixed, from the original "Attention Is All You Need" paper)
PE(pos, 2i)   = sin(pos / 10000 ^ (2i / d_model))
PE(pos, 2i+1) = cos(pos / 10000 ^ (2i / d_model))

# Learned (GPT-style, trainable parameters)
position_embedding = nn.Embedding(context_length, d_model)
```

---

### Causal Masking in Decoder-Only Models

For text generation, a model must only use past tokens to predict the next one â€” it cannot "cheat" by looking ahead. The causal mask is an upper-triangular matrix of `-inf` values applied before the softmax:

```python
mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1) * float('-inf')
attention_scores = attention_scores + mask
# Positions above the diagonal become -inf â†’ softmax â†’ 0 attention weight
```

---

### Scaled Dot-Product Attention

The core operation of every Transformer block:

```
Attention(Q, K, V) = softmax( QKáµ€ / âˆšd_k ) Â· V
```

The `âˆšd_k` scaling term prevents dot products from growing too large in high-dimensional spaces, which would push the softmax into regions with near-zero gradients â€” making training unstable.

---

### Multi-Head Attention

Rather than computing one set of attention weights, multi-head attention runs `h` independent attention functions in parallel, each with its own learned projection matrices:

```python
# Each head learns to attend to different types of relationships
head_1: attends to syntactic dependencies  (subject-verb agreement)
head_2: attends to semantic relationships  (word meaning similarity)
head_3: attends to positional patterns     (phrase boundaries)
...
# Outputs are concatenated and projected back to d_model dimensions
```

---

## ğŸ› ï¸ Tech Stack

| Tool | Purpose |
|------|---------|
| **Python 3.9+** | Core language |
| **PyTorch 2.x** | Tensor operations, `nn.Module` implementations |
| **tiktoken** | OpenAI's fast BPE tokenizer |
| **HuggingFace Transformers** | Reference models and tokenizers |
| **NumPy** | Numerical operations and array manipulation |
| **Matplotlib** | Embedding space visualizations |
| **Jupyter Lab** | Interactive notebook environment |

---

## ğŸ“Š Learning Path

This notebook fits naturally into the following progression:

```
[ Python & NumPy Basics ]
          â†“
[ NLP Fundamentals â€” Bag of Words, TF-IDF ]
          â†“
[ THIS NOTEBOOK â€” Tokenization Â· Embeddings Â· Transformer Internals ]
          â†“
[ Fine-tuning Pre-trained LLMs with HuggingFace ]
          â†“
[ Building RAG Systems with Vector Databases ]
          â†“
[ LoRA / QLoRA for Efficient Fine-tuning ]
```

**Recommended prerequisites:**
- Basic Python and NumPy
- High school linear algebra (matrix multiplication, dot products)
- Familiarity with what a neural network is (loss function, backpropagation)

**What to explore next:**
- Fine-tuning pre-trained models with HuggingFace `Trainer` API
- Building a RAG pipeline with Pinecone or Chroma as the vector store
- Training a small GPT-2-scale model from scratch using the pipeline built here

---

## ğŸ“š References & Further Reading

| Resource | Description |
|----------|-------------|
| [Attention Is All You Need](https://arxiv.org/abs/1706.03762) | The original Transformer paper â€” Vaswani et al., 2017 |
| [GPT-2 Paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) | Language models as unsupervised multitask learners |
| [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) | Jay Alammar's definitive visual walkthrough |
| [The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/) | Harvard NLP â€” code-first Transformer implementation |
| [tiktoken](https://github.com/openai/tiktoken) | OpenAI's fast BPE tokenizer |
| [LLMs from Scratch](https://github.com/rasbt/LLMs-from-scratch) | Sebastian Raschka's companion repository |
| [HuggingFace NLP Course](https://huggingface.co/learn/nlp-course) | End-to-end NLP course with Transformers |

---

## ğŸ‘¨â€ğŸ’» Author

<div align="center">

**Minhaj**

[![GitHub](https://img.shields.io/badge/GitHub-Minhaj078-181717?style=flat-square&logo=github)](https://github.com/Minhaj078)

*Python & Full Stack Development Â· Lovely Professional University*

</div>

---

## ğŸ“„ License

This project is licensed under the **MIT License** â€” you are free to use, adapt, and build upon this work with attribution.

---

<div align="center">

**If this notebook helped you understand LLMs better, give it a â­ â€” it helps others find it too.**

<br/>

*Built with curiosity and an unreasonable number of print statements.*

</div>
