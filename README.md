# ğŸ§  Mini GPT From Scratch (PyTorch)

This project implements a minimal GPT-style language model completely from scratch using PyTorch.

It covers the full pipeline:
- Custom tokenizer
- Dataset creation (next-token prediction)
- Self-attention mechanism
- GPT architecture
- Training loop
- Text generation
- Model saving

---

## ğŸš€ Features

âœ” Custom word-level tokenizer  
âœ” Sliding window dataset for next-token prediction  
âœ” Multi-head self-attention  
âœ” Positional embeddings  
âœ” Training on custom text  
âœ” Text generation with sampling  
âœ” Model checkpoint saving  

---

## ğŸ“ Project Structure

```text
LLM-Data-Preprocessing-Embedding-Architecture/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ input.txt
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ tokenizer.py
â”‚   â”œâ”€â”€ dataset.py
â”‚   â”œâ”€â”€ model.py
â”‚   â””â”€â”€ train.py
â”‚
â”œâ”€â”€ gpt_model.pth
â””â”€â”€ README.md
```

---

## ğŸ— Model Architecture

- Token Embedding
- Positional Embedding
- Self-Attention Layer
- LayerNorm
- Linear Output Layer

This model predicts the next token given previous context.

---

## â–¶ï¸ How to Run

1. Install dependencies:
   ```pip install torch numpy```

2. Add training text inside:
  ```data/input.txt```

3. Train the model:
   ```python src/train.py```

4. Generated text will be printed in terminal.
  
5. Model weights will be saved as:

---

## ğŸ“Š Sample Output
```
Epoch 0 | Loss: 3.77
Epoch 4 | Loss: 3.11

=== Text Generation ===
project scratch use builds sentence from in data .
```

---

## ğŸ¯ Learning Outcomes

This project demonstrates understanding of:

- Transformer architecture fundamentals
- Self-attention implementation
- Language modeling
- PyTorch training workflows
- End-to-end NLP pipeline development

---

## ğŸ”¥ Future Improvements

- Add multiple transformer blocks
- Add temperature sampling
- Add model loading script
- Train on larger dataset
- Add evaluation metrics (Perplexity)

---

## ğŸ‘¨â€ğŸ’» Author

Minhaj Ahmad  
Built as part of LLM deep learning exploration.

